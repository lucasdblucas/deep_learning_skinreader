{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1qTFUcbyByfj9Lw9Nomzt8PSiQEXiAsMt",
      "authorship_tag": "ABX9TyNUkbCZ37dp3cT+gYt9OmHz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fc778460cc6b4a62bea97546f16da54a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a911261923394d3fbb7e5c97231786c7",
              "IPY_MODEL_659e9daed75e41739ec4a1295275dec1",
              "IPY_MODEL_37e7ac0a08454ea1abc62367209437cd"
            ],
            "layout": "IPY_MODEL_b7fbb39229d4462a80de5cb35ef27341"
          }
        },
        "a911261923394d3fbb7e5c97231786c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfa0eb9fb4d04aec9278768d1b6e4cdd",
            "placeholder": "​",
            "style": "IPY_MODEL_de2df4d18b614ea6962ec61b0b5c7408",
            "value": "100%"
          }
        },
        "659e9daed75e41739ec4a1295275dec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dd434caaccb4f2c903cbbee7460c39e",
            "max": 102502400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7c0b05a2b334c9282b60c10bb82e4e8",
            "value": 102502400
          }
        },
        "37e7ac0a08454ea1abc62367209437cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_173df3e895dd4e7ab54c9c353b84e721",
            "placeholder": "​",
            "style": "IPY_MODEL_93806b766fb04ebb916cc131a98ca8e3",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 239MB/s]"
          }
        },
        "b7fbb39229d4462a80de5cb35ef27341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfa0eb9fb4d04aec9278768d1b6e4cdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de2df4d18b614ea6962ec61b0b5c7408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4dd434caaccb4f2c903cbbee7460c39e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c0b05a2b334c9282b60c10bb82e4e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "173df3e895dd4e7ab54c9c353b84e721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93806b766fb04ebb916cc131a98ca8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de46d819bcee43fd9be041ce44e021a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7697eddb7e4c478fbb5f1c9499a1c8bb",
              "IPY_MODEL_918d64379b65418c92ff925e88c3a59f",
              "IPY_MODEL_cf33f188a85743ca82f017c36d963425"
            ],
            "layout": "IPY_MODEL_31c7eee3e8ea412691ff496528bd3a89"
          }
        },
        "7697eddb7e4c478fbb5f1c9499a1c8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23844d331cd347a386c974565417bfa2",
            "placeholder": "​",
            "style": "IPY_MODEL_ae98c2557e2a4a13bc0db0ec0984b704",
            "value": "100%"
          }
        },
        "918d64379b65418c92ff925e88c3a59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68d788d1f43f471ba91df8b665f7cc5c",
            "max": 102502400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc496956bff84473b95b3800e4baae06",
            "value": 102502400
          }
        },
        "cf33f188a85743ca82f017c36d963425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae6770b09b494bc2a83e82d9a4f47a3e",
            "placeholder": "​",
            "style": "IPY_MODEL_42e6611f29b44734ab34def1c560a1d1",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 69.2MB/s]"
          }
        },
        "31c7eee3e8ea412691ff496528bd3a89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23844d331cd347a386c974565417bfa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae98c2557e2a4a13bc0db0ec0984b704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68d788d1f43f471ba91df8b665f7cc5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc496956bff84473b95b3800e4baae06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae6770b09b494bc2a83e82d9a4f47a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42e6611f29b44734ab34def1c560a1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasdblucas/deep_learning_skinreader/blob/master/BARBIERI_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# código base -> https://github.com/microsoft/leprosy-skin-lesion-ai-analysis"
      ],
      "metadata": {
        "id": "hQgw_P-5Tu-Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretrainedmodels==0.7.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icdhIaD963fV",
        "outputId": "6a72c81a-a29d-42a9-a849-e4bea3d5a43d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from pretrainedmodels==0.7.4) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from pretrainedmodels==0.7.4) (0.14.0+cu116)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pretrainedmodels==0.7.4) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from munch->pretrainedmodels==0.7.4) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->pretrainedmodels==0.7.4) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->pretrainedmodels==0.7.4) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->pretrainedmodels==0.7.4) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->pretrainedmodels==0.7.4) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->pretrainedmodels==0.7.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->pretrainedmodels==0.7.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->pretrainedmodels==0.7.4) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->pretrainedmodels==0.7.4) (2.10)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60966 sha256=e5180757246d9e93c995a2d6ed9463de17525550415d6b8d0e73e7156bbaec83\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/fa/b9/5c82b59d905f95542a192b883c0cc0082407ea2f54beb2f9e6\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U4JkE-tv5ftw"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "# Licensed under the MIT License.\n",
        "import copy\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import pretrainedmodels as ptm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "from random import randint\n",
        "\n",
        "# específico do prepare data\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "import torch\n",
        "\n",
        "from datetime import datetime\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Support"
      ],
      "metadata": {
        "id": "c1itEdTplBRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_path = os.path.join('/content/drive/MyDrive/Colab Notebooks/dataset/skinreader/BDHansen', 'metadados')\n",
        "images_closeup_path = os.path.join('/content/drive/MyDrive/Colab Notebooks/dataset/skinreader/BDHansen', 'closeup')\n",
        "images_all_path = os.path.join('/content/drive/MyDrive/Colab Notebooks/dataset/skinreader/BDHansen', 'all_closeup_panoramic')\n",
        "images_panoramic_path = os.path.join('/content/drive/MyDrive/Colab Notebooks/dataset/skinreader/BDHansen', 'panoramic')"
      ],
      "metadata": {
        "id": "a2fZ4lNblAjI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataframe from .json"
      ],
      "metadata": {
        "id": "-2p7elb-jMmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_jsonfile(json_path):\n",
        "    with open(json_path,'r') as arquivo:\n",
        "        paciente = arquivo.read()\n",
        "        return json.loads(paciente)"
      ],
      "metadata": {
        "id": "1dGcqSsMoqdV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_json_files():\n",
        "    files = os.listdir(metadata_path)\n",
        "    return [load_jsonfile(os.path.join(metadata_path, f)) for f in files if f.endswith('.json')]"
      ],
      "metadata": {
        "id": "E0O_jE6EkUlf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(json_list):\n",
        "    patient_id_list = []\n",
        "    image_list_for_patient = []\n",
        "    for j in json_list:\n",
        "        patient_id_list.append(j['_via_settings']['project']['name'].split('/')[0])\n",
        "        list_aux = []\n",
        "        for key, value in j['_via_img_metadata'].items():\n",
        "            if len(value['regions']) != 0 :\n",
        "                # print(value['filename'])\n",
        "                # print(value['regions'][0])\n",
        "                list_aux.append((value['filename'], value['regions'][0]['region_attributes']['leprosy']))\n",
        "        if len(list_aux) != 0: image_list_for_patient.append(list_aux) #lista de imagens e diagnóstico para cada imagem para cada paciente\n",
        "    \n",
        "    dados = []\n",
        "    c_imagenames = os.listdir(images_closeup_path)\n",
        "    a_imagenames = os.listdir(images_all_path)\n",
        "    p_imagenames = os.listdir(images_panoramic_path)\n",
        "    print(f'Numero de imagens -> \\nColseup: {len(c_imagenames)}\\nAll: {len(a_imagenames)}\\nPanoramicas: {len(p_imagenames)}')\n",
        "    for index, p in enumerate(patient_id_list):\n",
        "        study_id = p.split('-')[1]\n",
        "\n",
        "        c_images = [image_name for image_name in c_imagenames if study_id in image_name]\n",
        "        a_images = [image_name for image_name in a_imagenames if study_id in image_name]\n",
        "        p_images = [image_name for image_name in p_imagenames if study_id in image_name]\n",
        "\n",
        "        # for i in c_images:\n",
        "        #     no_extention = i.split('.')[0]\n",
        "        #     for n in image_list_for_patient[index]:\n",
        "        #         if no_extention in n[0]:\n",
        "        #             dados.append([\n",
        "        #                 study_id, # ID do estudo\n",
        "        #                 p, # ID do paciente\n",
        "        #                 i, # nome da imagem\n",
        "        #                 n[0], # filename da imagem no json\n",
        "        #                 os.path.join(images_closeup_path, i), # path da imagem local\n",
        "        #                 n[1], # dignostico por imagem no json\n",
        "        #                 'closeup', # tipo de imagem de acordo com a distribuição local\n",
        "        #                 i.split('.')[0].split('-')[-1] # tipo de imagem de acordo com o estudo\n",
        "        #             ])\n",
        "                    \n",
        "        # for i in p_images: \n",
        "        #     no_extention = i.split('.')[0]\n",
        "        #     for n in image_list_for_patient[index]:\n",
        "        #         if no_extention in n[0]:\n",
        "        #             dados.append([study_id, p, i, n[0], os.path.join(images_closeup_path, i), n[1], 'panoramic', i.split('.')[0].split('-')[-1]])\n",
        "\n",
        "        for i in a_images:\n",
        "            no_extention = i.split('.')[0]\n",
        "            for n in image_list_for_patient[index]:\n",
        "                if no_extention in n[0]:\n",
        "                    dados.append([study_id, p, no_extention, n[0], os.path.join(images_closeup_path, i), n[1], 'all', i.split('.')[0].split('-')[-1]])\n",
        "\n",
        "    return dados"
      ],
      "metadata": {
        "id": "-3f4ojL83tVI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clean"
      ],
      "metadata": {
        "id": "j7Uoemhv7Xls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Clean poly_df\n",
        "return a dataframe with columns ['patient_id','image_name','patient_leprosy','type']\n",
        "'''\n",
        "def clean_poly(dir_name, # os.path.join(dir_name, 'images') contains all the lesion images\n",
        "               poly_address, # address of labels.tsv\n",
        "               patient_info_address, # address of patient information csv\n",
        "               keep_label_id=False # whether to include the column 'label_id' in the output dataframe\n",
        "              ):\n",
        "    poly_df = pd.read_csv(poly_address, sep=\"\\t\")\n",
        "    patient_df=pd.read_csv(patient_info_address, engine='python').replace(\"200AF\", \"200AP\")\n",
        "    all_images=os.listdir(os.path.join(dir_name, 'images'))\n",
        "    \n",
        "    # These three patients dropped the exp. \n",
        "    poly_df=poly_df[~poly_df.patient_id.isin(\n",
        "        [\"Patient-051GR\", \"Patient-121DF\",\"Patient-203PR\"])]\n",
        "    \n",
        "    # Drop those rows if corresponding images were not found in the image folder\n",
        "    poly_df=poly_df[poly_df.image_name.isin(all_images)]\n",
        "    # StudyID é somente a identificação do estudo, ou seja se patiente_id = Patient-051GR, StudyID = 051GR\n",
        "    StudyID=[str[8:] for str in poly_df['patient_id'].values]\n",
        "\n",
        "    Diag=[patient_df.loc[patient_df['StudyID']==id,'Diagnostic'].values[0] for id in StudyID]\n",
        "    Diag_patient=['leprosy' if x<2 else 'other_dermotosis' for x in Diag]\n",
        "    # Diagnóstico de lepra ou outra dermatite\n",
        "    poly_df['patient_leprosy']=Diag_patient\n",
        "    \n",
        "    #Remove the lesion if diagnostic result of the patient did not match that of the lesion when the lesion diagnostic result is present. \n",
        "    rm_label_id=poly_df.loc[(poly_df['patient_leprosy']!=poly_df['lesion_leprosy']) & poly_df['lesion_leprosy'].notna(),\"label_id\"].values\n",
        "    if len(rm_label_id)>0: print(\"Drop these ids because patient_diag is not equal to lesion_diag: \"+', '.join(rm_label_id))\n",
        "    poly_df=poly_df.loc[~ poly_df['label_id'].isin(rm_label_id),]\n",
        "    poly_df.drop(columns=['lesion_leprosy'],inplace=True)\n",
        "    \n",
        "    # Three image types: closeup, panoramic and edge\n",
        "    types=[x.split('.')[0].split('-')[-1] for x in poly_df.image_name]\n",
        "    poly_df['type']=types\n",
        "    poly_df.replace('paoramic','panoramic',inplace=True)\n",
        "    poly_df.replace('panoramis','panoramic',inplace=True)\n",
        "    if keep_label_id: return poly_df[['patient_id','image_name','label_id','patient_leprosy','type']].drop_duplicates()\n",
        "    else:   \n",
        "        poly_df=poly_df[['patient_id','image_name','patient_leprosy','type']]\n",
        "        return poly_df.drop_duplicates()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NHvv_Zh77SLY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reviwed\n",
        "#return all the image ids of one selected patient\n",
        "# Documentation pd.loc : https://medium.com/horadecodar/data-science-tips-02-como-usar-loc-e-iloc-no-pandas-fab58e214d87#:~:text=loc%3A,aos%20demais%20usos%2C\n",
        "def get_img_ids_onepatient(poly_df,patient_id,type_spec=None):\n",
        "    df=poly_df\n",
        "\n",
        "    # se \"type_spec\" for especificado, então df é modificado\n",
        "    # df é modificado para ser um dataframe somente com o \"type\" do tipo \"type_spec\".\n",
        "    # df.loc -> retornar as linhas onde a coluna type é igual a type_spec, sem especificar colunas, ou seja, retorna todas as colunas\n",
        "    if type_spec: \n",
        "        df=df.loc[df.type==type_spec,]\n",
        "\n",
        "    # somente os valores da coluna \"image_name\" para todos os pacientes.\n",
        "    selected_label_ids=df.loc[df.patient_id==patient_id,].image_name.values\n",
        "    # selected_label_ids=df.loc[df.patient_id==patient_id,].image_path.values # estou supondo que o nome da imagem seja o seu path.\n",
        "    return selected_label_ids.tolist()\n",
        "\n",
        "#return all the image ids of the selected patients\n",
        "# são todas as imagens. Para cada paciente existem várias imagens.\n",
        "def get_img_ids(poly_df,patient_ids,type_spec=None):\n",
        "    selected_label_ids=sum([get_img_ids_onepatient(poly_df,patient,type_spec) for patient in patient_ids],[])\n",
        "    return selected_label_ids"
      ],
      "metadata": {
        "id": "68pgDhoISYlv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reviwed\n",
        "'''\n",
        "split a sequence 'seq' into 'num' sub-sequence\n",
        "return a list including all the sub-sequences [seq1,...,seq_num] \n",
        "'''\n",
        "# Retorna uma lista (out) de listas. Cada lista dentro de \"out\" é uma branck com pacientes dentro.\n",
        "def chunkIt(seq, num):\n",
        "    avg = int(len(seq) / num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "    while len(out)<num:\n",
        "        if len(out)<num-1: out.append(seq[int(last):int(last + avg)])\n",
        "        else: out.append(seq[int(last):len(seq)])\n",
        "        last += avg\n",
        "    return out"
      ],
      "metadata": {
        "id": "oagEEWRJLUWE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess"
      ],
      "metadata": {
        "id": "C--_mMxn7xIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BARBIERE;2022 Random Transformations"
      ],
      "metadata": {
        "id": "5fDdYRzcjLqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reviwed\n",
        "# size = igual as dimensões da camada de input\n",
        "def barbiere2022_random_derive_transform(size, mean, std, scale=.6, change_aspect_ratio=None):\n",
        "    augs={\n",
        "        \"size\":0,\n",
        "        \"mean\":0,\n",
        "        \"std\":0,\n",
        "       \"color_contrast\": 0.3, \n",
        "       \"color_saturation\": 0.3, \n",
        "       \"color_brightness\": 0.3, \n",
        "       \"color_hue\": 0.1, \n",
        "       \"rotation\": 90, \n",
        "       \"shear\": 20\n",
        "    }\n",
        "    augs['size'] = size\n",
        "    augs['mean'] = mean\n",
        "    augs['std'] = std\n",
        "    tf_list = []\n",
        "    # RandomResizedCrop is doing a crop first and then scale to the desired size.\n",
        "    # [ref: https://pytorch.org/vision/main/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py]\n",
        "    if not change_aspect_ratio:  tf_list.append(transforms.RandomResizedCrop(augs['size'], scale=(scale, 1), ratio=(1.0, 1.0))) #mantem ratio    \n",
        "    else: tf_list.append(transforms.RandomResizedCrop(augs['size'], scale=(scale, 1)))\n",
        "    tf_list.append(transforms.RandomHorizontalFlip())\n",
        "    tf_list.append(transforms.RandomVerticalFlip())\n",
        "    tf_list.append(\n",
        "        transforms.ColorJitter(\n",
        "            brightness=augs['color_brightness'],\n",
        "            contrast=augs['color_contrast'],\n",
        "            saturation=augs['color_saturation'],\n",
        "            hue=augs['color_hue']\n",
        "        )\n",
        "    )\n",
        "    tf_list.append(transforms.ToTensor())\n",
        "    tf_augment = transforms.Compose(tf_list)\n",
        "\n",
        "    train_tf=transforms.Compose([tf_augment,transforms.Normalize(augs['mean'], augs['std'])])\n",
        "    orig_tf=transforms.Compose([transforms.Resize((augs['size'],augs['size'])),transforms.ToTensor(),transforms.Normalize(augs['mean'], augs['std'])])\n",
        "    return train_tf,orig_tf"
      ],
      "metadata": {
        "id": "EzwV5-ZCjIB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BARBIERE;2022 Preset Transformations"
      ],
      "metadata": {
        "id": "rcOwas5cjbX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Reviwed\n",
        "# # size = igual as dimensões da camada de input\n",
        "# def barbiere2022_preset_derive_transform(size, mean, std, scale=.6, change_aspect_ratio=None):\n",
        "#     augs={\n",
        "#         \"size\":0,\n",
        "#         \"mean\":0,\n",
        "#         \"std\":0,\n",
        "#        \"color_contrast\": 0.3, \n",
        "#        \"color_saturation\": 0.3, \n",
        "#        \"color_brightness\": 0.3, \n",
        "#        \"color_hue\": 0.1, \n",
        "#        \"rotation\": 90, \n",
        "#        \"shear\": 20\n",
        "#     }\n",
        "#     augs['size'] = size\n",
        "#     augs['mean'] = mean\n",
        "#     augs['std'] = std\n",
        "#     tf_list = []\n",
        "#     # RandomResizedCrop is doing a crop first and then scale to the desired size.\n",
        "#     # [ref: https://pytorch.org/vision/main/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py]\n",
        "#     if not change_aspect_ratio:  tf_list.append(transforms.RandomResizedCrop(augs['size'], scale=(scale, 1), ratio=(1.0, 1.0))) #mantem ratio    \n",
        "#     else: tf_list.append(transforms.RandomResizedCrop(augs['size'], scale=(scale, 1)))\n",
        "#     tf_list.append(transforms.RandomHorizontalFlip())\n",
        "#     tf_list.append(transforms.RandomVerticalFlip())\n",
        "#     tf_list.append(\n",
        "#         transforms.ColorJitter(\n",
        "#             brightness=augs['color_brightness'],\n",
        "#             contrast=augs['color_contrast'],\n",
        "#             saturation=augs['color_saturation'],\n",
        "#             hue=augs['color_hue']\n",
        "#         )\n",
        "#     )\n",
        "#     tf_list.append(transforms.ToTensor())\n",
        "#     tf_augment = transforms.Compose(tf_list)\n",
        "\n",
        "#     train_tf=transforms.Compose([tf_augment,transforms.Normalize(augs['mean'], augs['std'])])\n",
        "#     orig_tf=transforms.Compose([transforms.Resize((augs['size'],augs['size'])),transforms.ToTensor(),transforms.Normalize(augs['mean'], augs['std'])])\n",
        "#     return train_tf,orig_tf"
      ],
      "metadata": {
        "id": "gpVP_Dr57zKu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLARO;2022 Random Transformations"
      ],
      "metadata": {
        "id": "Pj0BaYQokmSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reviwed\n",
        "# size = igual as dimensões da camada de input\n",
        "def claro2022_random_derive_transform(transformations, size, mean, std, scale=.6, change_aspect_ratio=None):\n",
        "    augs={\n",
        "        \"size\":0,\n",
        "        \"mean\":0,\n",
        "        \"std\":0,\n",
        "       \"color_contrast\": 0.3, \n",
        "       \"color_saturation\": 0.3, \n",
        "       \"color_brightness\": 0.3, \n",
        "       \"color_hue\": 0.1, \n",
        "       \"rotation\": 90, \n",
        "       \"shear\": 20\n",
        "    }\n",
        "    augs['size'] = size\n",
        "    augs['mean'] = mean\n",
        "    augs['std'] = std\n",
        "    tf_list = []\n",
        "    # [ref: https://pytorch.org/vision/main/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py]\n",
        "    # RandomRotation\n",
        "    if transformations[\"rotation\"]: tf_list.append(transforms.RandomRotation(degrees=(0, 180)))\n",
        "    # RandomTranslation\n",
        "    if transformations[\"translation\"]: tf_list.append(transforms.RandomAffine(degrees=0, translate=(0.35, 0.35)))\n",
        "    # RandomShear\n",
        "    if transformations[\"shear\"]: tf_list.append(transforms.RandomAffine(degrees=0, shear=[-50, 50, -50, 50]))\n",
        "    # RandomFlip\n",
        "    if transformations[\"flip\"]:\n",
        "        tf_list.append(transforms.RandomHorizontalFlip())\n",
        "        tf_list.append(transforms.RandomVerticalFlip())\n",
        "    # RandomZoom\n",
        "    if transformations[\"zoom\"]: tf_list.append(transforms.RandomResizedCrop(augs['size'], scale=(scale, 1), ratio=(1.0, 1.0))) #mantem ratio    \n",
        "    \n",
        "    tf_list.append(transforms.ToTensor())\n",
        "    tf_augment = transforms.Compose(tf_list)\n",
        "\n",
        "    # Resize, to Tensor e Normalização\n",
        "    train_tf=transforms.Compose([tf_augment,transforms.Normalize(augs['mean'], augs['std'])])\n",
        "    orig_tf=transforms.Compose([transforms.Resize((augs['size'],augs['size'])),transforms.ToTensor(),transforms.Normalize(augs['mean'], augs['std'])])\n",
        "    return train_tf, orig_tf"
      ],
      "metadata": {
        "id": "IJDMrhTCkk1S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLARO;2022 Preset Transformations"
      ],
      "metadata": {
        "id": "y4NZHS_jk9Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reviwed\n",
        "# size = igual as dimensões da camada de input\n",
        "def claro2022_preset_derive_transform(transformations, size, mean, std, scale=.6, change_aspect_ratio=None):\n",
        "    augs={\n",
        "        \"size\":0,\n",
        "        \"mean\":0,\n",
        "        \"std\":0,\n",
        "       \"color_contrast\": 0.3, \n",
        "       \"color_saturation\": 0.3, \n",
        "       \"color_brightness\": 0.3, \n",
        "       \"color_hue\": 0.1, \n",
        "       \"rotation\": 90, \n",
        "       \"shear\": 20\n",
        "    }\n",
        "    augs['size'] = size\n",
        "    augs['mean'] = mean\n",
        "    augs['std'] = std\n",
        "    tf_list = []\n",
        "    # [ref: https://pytorch.org/vision/main/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py]\n",
        "    # RandomRotation\n",
        "    tf_list.append(transforms.RandomRotation(degrees=(0, 180)))\n",
        "    # RandomTranslation\n",
        "    tf_list.append(transforms.RandomAffine(degrees=0, translation=(0.35, 0.35)))\n",
        "    # RandomShear\n",
        "    tf_list.append(transforms.RandomAffine(degrees=0, shear=[-50, 50, -50, 50]))\n",
        "    # RandomFlip\n",
        "    tf_list.append(transforms.RandomHorizontalFlip())\n",
        "    tf_list.append(transforms.RandomVerticalFlip())\n",
        "    # RandomZoom\n",
        "    tf_list.append(transforms.RandomResizedCrop(augs['size'], scale=(scale, 1), ratio=(1.0, 1.0))) #mantem ratio    \n",
        "    \n",
        "    tf_augment = transforms.Compose(tf_list)\n",
        "\n",
        "    # Resize, to Tensor e Normalização\n",
        "    train_tf=transforms.Compose([tf_augment,transforms.Normalize(augs['mean'], augs['std'])])\n",
        "    orig_tf=transforms.Compose([transforms.Resize((augs['size'],augs['size'])),transforms.ToTensor(),transforms.Normalize(augs['mean'], augs['std'])])\n",
        "    return train_tf, orig_tf"
      ],
      "metadata": {
        "id": "exVFIEyzk7gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare Data"
      ],
      "metadata": {
        "id": "deU3C7h-8CK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeprosyDataset(Dataset):\n",
        "    \"\"\"Leprosy dataset.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        img_names, \n",
        "        root_dir, \n",
        "        img_df,\n",
        "        target_name, \n",
        "        transform=None,\n",
        "        id_col='label_id',\n",
        "        extension=\".jpg\"\n",
        "    ):\n",
        "        \n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_names (string): List of the names of images in this dataset not including extension\n",
        "            root_dir (string): Image directory.\n",
        "            img_df (DataFrame): Dataframe including patient_id, image_name, label_id, lesion_leprosy. Its label_id is identitical to the img_names\n",
        "            target_name (string): the colname of the response variable in img_df\n",
        "            transform (callable, optional): Transform to be applied on images.\n",
        "        Returns:\n",
        "            image: lesion image\n",
        "            target: 0(leprosy)/1(nonleprosy)\n",
        "            weight: prop to 1/(num of images of the patient) \n",
        "            freq_ratio:  1/(num of images of the patient) \n",
        "        \"\"\"\n",
        "        self.img_names = img_names\n",
        "        self.root_dir = root_dir\n",
        "        self.img_df=img_df\n",
        "        self.transform = transform\n",
        "        self.target_name=target_name\n",
        "        classes = list(self.img_df[self.target_name].unique())\n",
        "        classes.sort()\n",
        "        self.class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "        self.classes = classes\n",
        "        self.extension=extension\n",
        "        self.id_col=id_col\n",
        "        lesion_count=[self.img_df.patient_id.values.tolist().count(p) for p in self.img_df.patient_id] # count num of images for each patient\n",
        "        count_max=max(lesion_count)# max num of images of one patient\n",
        "        self.count_max=count_max\n",
        "        self.img_df['sample_weight']=[count_max/x for x in lesion_count] # give an option to re-weight the loss if there are some patients with siginificantly more images than the other. \n",
        "        self.img_df['sample_freq_ratio']=[1/x for x in lesion_count]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        image = Image.open(\n",
        "            os.path.join(\n",
        "                self.root_dir,\n",
        "                self.img_names[i]+self.extension\n",
        "            )\n",
        "        )\n",
        "        weight=self.img_df.loc[self.img_df[self.id_col]==self.img_names[i],'sample_weight'].values[0]\n",
        "        freq_ratio=self.img_df.loc[self.img_df[self.id_col]==self.img_names[i],'sample_freq_ratio'].values[0]\n",
        "        target = self.class_to_idx[self.img_df.loc[self.img_df[self.id_col]==self.img_names[i],self.target_name].values[0]]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, target,weight,freq_ratio\n",
        "    \n",
        "    def numpatient(self):\n",
        "        return len(set(self.img_df.loc[self.img_df[self.id_col].isin(self.img_names),'patient_id'].values))\n"
      ],
      "metadata": {
        "id": "w740JXFx8D6w"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run Model"
      ],
      "metadata": {
        "id": "lP5s1_Uf72A-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(\n",
        "    save_dir, # models and result tables will saved in 'save_dir/experiment_name'. 'save_dir' must exists!\n",
        "    dir_name, # all images are in dir_name/dataset_name\n",
        "    \n",
        "    # poly_address, # address of labels.tsv\n",
        "    # patient_info_address, # address of the patient form csv\n",
        "    poly_df,\n",
        "\n",
        "    id=0, # cuda id for parallel experiments\n",
        "    experiment_name = None, # if None, experiment_name will be the date when exp is finished\n",
        "    lr=.003, # learning rate\n",
        "    wd=0, # weight decay\n",
        "    batch_size=32,\n",
        "    tuning='tune_all', # default is to train the whole neural net. Otherwise train the last layer only. \n",
        "    class_weight=\"no\", # not apply class_weight to the loss. Otherwise do apply. \n",
        "    sample_weight=\"no\",# not apply sample_weight to the loss. Otherwise do apply. \n",
        "    num_epoch=200, # num of epochs\n",
        "    model_name='resnet50',# the other option 'inceptionv4'\n",
        "    transformations=None,\n",
        "    no_augmentation=False, # if TRUE -> no augmentation at all\n",
        "    change_aspect_ratio=None, # whether to change aspect ratio\n",
        "    scale=.6, # scaling factor for data augmentation\n",
        "\n",
        "    dataset_name='images', # all images are in dir_name/dataset_name\n",
        "    type_spec=None,# If None, use all three types of images. If ='closeup'/'panoramic'/'edge', only use the specific type of images for model training/validation/testing.\n",
        "    optim_type=\"SGD\", # otherwise use \"Adam\" as the optimizer\n",
        "    num_fold=6, # Split all patients into 6 folds. The last folds will be used to validate Model 3 later. The first 5 folds will be used for 5-fold cross-validation. \n",
        "    test_foldid=0 # index of the validation fold. 0-4 only.\n",
        "):\n",
        "    if type_spec not in [None,'closeup','panoramic','edge']: raise IOError('Invalid image type!')\n",
        "    if not os.path.exists(save_dir): raise IOError('save_dir not exisit!')\n",
        "    device=torch.device(\"cuda\",id)\n",
        "    # device=torch.cuda.device('cuda:0')\n",
        "\n",
        "    # poly_df = clean_poly(\n",
        "    #     dir_name,\n",
        "    #     poly_address,\n",
        "    #     patient_info_address\n",
        "    # )\n",
        "\n",
        "    # A Index.unique() função Pandas retorna valores únicos no índice. Os únicos são retornados em ordem de aparência, isso NÃO classifica.\n",
        "    # patient_list - lista com pacientes sem pacientes repetidos.\n",
        "    patient_list = poly_df.patient_id.unique()\n",
        "    # patient_list_leprosy = retorna 'patient_id' dos pacientes com diagnóstico = 'leprosy', sem 'pacient_id' repetido. Na sequencia em que aparecem.\n",
        "    patient_list_leprosy = poly_df.loc[poly_df.patient_leprosy=='leprosy','patient_id'].unique()\n",
        "    # patiente_list_OD = retorna 'patient_id' dos pacientes com diagnóstico = 'other_dermotosis', sem 'pacient_id' repetido. Na sequencia em que aparecem.\n",
        "    # OD = Other Dermatosis\n",
        "    patient_list_OD = poly_df.loc[poly_df.patient_leprosy=='other_dermotosis','patient_id'].unique()\n",
        "    \n",
        "    # leprosy_split = patients with leprosy plited in chunks/batchs\n",
        "    leprosy_split=chunkIt(patient_list_leprosy, num_fold)\n",
        "    # OD_split = patients with outher dermatosis in chunks/batchs\n",
        "    OD_split=chunkIt(patient_list_OD, num_fold)\n",
        "\n",
        "    # Levae the last folder out for validating Model 3\n",
        "    leprosy_split.pop()\n",
        "    OD_split.pop()\n",
        "\n",
        "    # Fold 'test_foldid' for testing, one for model selection by the best accuracy, and the rest for model training. This is a stratified split. \n",
        "    # Concatenar representantes das classes (leprosy, other dermatosis)\n",
        "    test_patient=np.concatenate((leprosy_split.pop(test_foldid), OD_split.pop(test_foldid) ), axis=None)\n",
        "    val_patient=np.concatenate((leprosy_split.pop(), OD_split.pop()), axis=None)\n",
        "    train_patient=np.concatenate(leprosy_split+OD_split)\n",
        "    \n",
        "    # Drop the three patients without any closeup image\n",
        "    # não sei se ainda está no dataset, mas será usado em caso ainda constem.\n",
        "    poly_df=poly_df[~poly_df.patient_id.isin(['Patient-009OS', 'Patient-017SG', 'Patient-076PR'])]\n",
        "    \n",
        "    # Get all the image ids of the test/val/train patients\n",
        "    test_img_ids=get_img_ids(poly_df,test_patient,type_spec)\n",
        "    train_img_ids=get_img_ids(poly_df,train_patient,type_spec)\n",
        "    val_img_ids=get_img_ids(poly_df,val_patient,type_spec)\n",
        "    # poly_df=poly_df.loc[poly_df.image_name.isin(train_img_ids+val_img_ids+test_img_ids)]\n",
        "    # isin -> verifica se o valor está dentro do dataframe dado. Retorna um dataframe do mesmo tamanho preenchido com Boobleanas.\n",
        "    # Documentação isin -> https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html\n",
        "    # Verificar se as imagens com os id's selecionados estão no dataframe.\n",
        "    poly_df=poly_df.loc[poly_df.image_name.isin(train_img_ids+val_img_ids+test_img_ids)]\n",
        "\n",
        "    # train_diag = diagnostico para imagens, onde as imagens estão presentes. Ou seja só retorna o dignótico caso a imagem \n",
        "    # exista essa imagem.\n",
        "    train_diag=poly_df.loc[poly_df.image_name.isin(train_img_ids),'patient_leprosy'].values\n",
        "    # Conta quantidade de diagnósticos e retorna o máximo entre eles.\n",
        "    max_cnt=max(train_diag.tolist().count('leprosy'),train_diag.tolist().count('other_dermotosis'))\n",
        "    # não vou verificar isso agora, porque clss_weight não é utilizado nesse momento.\n",
        "    # Ao que parece é: o número máximo entre diagnótico de leprosy e outra dermatosi, esse número divido por diagnótico de leprosy e depois dermatosi\n",
        "    if class_weight!=\"no\": class_weights=torch.FloatTensor([max_cnt/train_diag.tolist().count('leprosy'),\n",
        "        max_cnt/train_diag.tolist().count('other_dermotosis')]).to(device)\n",
        "\n",
        "\n",
        "    #escolha do modelo\n",
        "    # ptm = pretrainedmodels \n",
        "    if model_name=='resnet50': \n",
        "        model=ptm.resnet50(num_classes=1000, pretrained='imagenet') # uso de uma resnet pretreinada - imagenet\n",
        "        # model_test = copy.deepcopy(model)\n",
        "    else: \n",
        "        model = ptm.inceptionv4(num_classes=1000, pretrained='imagenet') # se não a resnet50, uso de módulos inception, provavelemnte googlenet - iamgenet\n",
        "        # model_test = copy.deepcopy(model)\n",
        "    # github - pretrained models for pytorch -> https://github.com/Cadene/pretrained-models.pytorch#modellast_linear\n",
        "    # mais sobre pretrained models -> https://discuss.pytorch.org/t/how-to-modify-the-final-fc-layer-based-on-the-torch-model/766/22?page=2\n",
        "    # last_linear é a ultima camada FC de \"model\"\n",
        "    # torck.nn -> These are the basic building blocks for graphs\n",
        "    # Linear -> Aplica uma transformação linear a entrada\n",
        "    model.last_linear = nn.Linear(model.last_linear.in_features, 2) #(input, output)\n",
        "    # Colocar modelo na memória\n",
        "    if torch.cuda.is_available():model.cuda(id)\n",
        "    print(f'type model: {type(model)}, \\nmodel.input_size[1/: {model.input_size[1]}\\n\\n')\n",
        "    # preprocessamento das imagens\n",
        "    # test_tf = é somente as transformações básicas. Rezise para o tamanhdo do input da rede. Transformação para tensor e normalização ao final.\n",
        "    # train_tf = são as transformações utilizadas no aumento dos dados.\n",
        "    do_tranformations = None\n",
        "    if transformations['type'] == 'r_claro': \n",
        "        do_tranformations = claro2022_random_derive_transform\n",
        "    if transformations['type'] == 'p_claro': \n",
        "        do_tranformations = claro2022_preset_derive_transform\n",
        "    if transformations['type'] == 'r_barbiere': \n",
        "        do_tranformations = barbiere2022_random_derive_transform\n",
        "    if transformations['type'] == 'p_barbiere': \n",
        "        # do_tranformations = barbiere2022_preset_derive_transform\n",
        "        pass\n",
        "    \n",
        "    train_tf, test_tf = do_tranformations(\n",
        "        transformations,\n",
        "        model.input_size[1], # A imagem sofrerá resize para o tamanho da entrada \n",
        "        model.mean, model.std, # valores que serão utilizados para a normalização\n",
        "        scale, # scala para transformaçõs \n",
        "        change_aspect_ratio # muda ou não a taxa de aspecto. Padrão - None\n",
        "    )\n",
        "    val_tf = test_tf\n",
        "\n",
        "    # quando não há aumento de dados, o tipo de transformação que deve ser feita é somente de normalização, a mesma aplicada aos testes.\n",
        "    if no_augmentation: train_tf = test_tf\n",
        "    img_address = os.path.join(dir_name, dataset_name)\n",
        "    # img_address = os.path.join(dir_name, dataset_name)\n",
        "\n",
        "  \n",
        "    # Includes only one type of images if specified\n",
        "    if type_spec: poly_df=poly_df.loc[poly_df.type==type_spec,]\n",
        "    # Definir dataset de treinamento\n",
        "    train_data = LeprosyDataset(\n",
        "        train_img_ids, \n",
        "        img_address, \n",
        "        poly_df,\n",
        "        target_name='patient_leprosy',\n",
        "        transform=train_tf,                 \n",
        "        id_col='image_name',\n",
        "        extension=\".jpg\"\n",
        "    )\n",
        "    train_patient_num = train_data.numpatient()\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        # num_workers=4\n",
        "        num_workers=2\n",
        "    )\n",
        "    # Definir dataset de validação\n",
        "    val_data = LeprosyDataset(\n",
        "        val_img_ids, \n",
        "        img_address, \n",
        "        poly_df,\n",
        "        'patient_leprosy',\n",
        "        val_tf,\n",
        "        id_col='image_name',\n",
        "        extension=\".jpg\"\n",
        "    )\n",
        "    val_patient_num = val_data.numpatient()\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        # num_workers=4\n",
        "        num_workers=2\n",
        "    )\n",
        "    # Definir dataset de teste\n",
        "    test_data = LeprosyDataset(\n",
        "        test_img_ids, \n",
        "        img_address, \n",
        "        poly_df,'patient_leprosy',\n",
        "        test_tf,\n",
        "        id_col='image_name',\n",
        "        extension=\".jpg\"\n",
        "    )\n",
        "    test_patient_num = test_data.numpatient()\n",
        "    test_loader = DataLoader(\n",
        "        test_data,\n",
        "        batch_size=batch_size,\n",
        "        # num_workers=4\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    # decidir se vai treinar toda a rede ou somente parte dela\n",
        "    if class_weight==\"no\":\n",
        "        criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    else: criterion = nn.CrossEntropyLoss(reduction='none', weight=class_weights)\n",
        "    if(optim_type==\"SGD\"):\n",
        "        if tuning=='tune_all':\n",
        "            optimizer = optim.SGD(\n",
        "                model.parameters(),\n",
        "                lr=lr,\n",
        "                weight_decay=wd\n",
        "            )\n",
        "        else: optimizer = optim.SGD(\n",
        "            model.last_linear.parameters(),\n",
        "            lr=lr,\n",
        "            weight_decay=wd\n",
        "            )\n",
        "    else:\n",
        "        if tuning=='tune_all': optimizer = optim.Adam(model.parameters(),lr=lr,weight_decay=wd)\n",
        "        else: optimizer = optim.Adam(model.last_linear.parameters(),lr=lr,weight_decay=wd)       \n",
        "    \n",
        "    \n",
        "    # definição de variáveis e começo do treinamento\n",
        "    train_loss, val_loss, test_loss=[],[],[]\n",
        "    best_val=1000.0\n",
        "    best_val_acc=0.0\n",
        "    for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        # model training\n",
        "        model.train()\n",
        "        for i,data in enumerate(train_loader, 0): # o argumento 0?\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels, weights, ratios = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            weights = weights.float()\n",
        "            weights = weights.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            if sample_weight != \"no\": \n",
        "                loss = loss * weights\n",
        "            loss.mean().backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.sum()\n",
        "        \n",
        "        confusion_matrix = torch.zeros(2,2)\n",
        "        correct, total, correct_p = 0, 0, 0\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():#Context-manager that disabled gradient calculation. \n",
        "                             #Disabling gradient calculation is useful for inference, when you are sure\n",
        "                             #that you will not call Tensor.backward(). It will reduce memory\n",
        "                            #consumption for computations that would otherwise have requires_grad=True\n",
        "            for data in train_loader:\n",
        "                images, labels, _, ratios = data\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                ratios = ratios.float()\n",
        "                ratios = ratios.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.sum()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                correct_p += ((predicted == labels).float() * ratios).sum().item() # patient_wise average\n",
        "                for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
        "                    confusion_matrix[t.long(), p.long()] += 1\n",
        "        print('[%d] loss: %.3f' %(epoch + 1, running_loss))\n",
        "        print('Accuracy of the network on the training images: %d %%' % (100 * correct / total))\n",
        "        print(correct_p/train_patient_num)\n",
        "        train_loss.append([running_loss.item(),100 * correct / total,100*correct_p/train_patient_num]+\n",
        "                      confusion_matrix.reshape(1,-1)[0].tolist())\n",
        "        \n",
        "        # model validation \n",
        "        confusion_matrix = torch.zeros(2,2)\n",
        "        correct,total,correct_p = 0,0,0\n",
        "        running_loss = 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                images, labels,_ ,ratios= data\n",
        "                images=images.to(device)\n",
        "                labels=labels.to(device)\n",
        "                ratios=ratios.float()\n",
        "                ratios=ratios.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.sum()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                correct_p += ((predicted == labels).float()*ratios).sum().item() \n",
        "                for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
        "                    confusion_matrix[t.long(), p.long()] += 1\n",
        "        print('Accuracy of the network on the val images: %d %%' % (100 * correct / total))\n",
        "        print(correct_p/val_patient_num)\n",
        "        val_loss.append([running_loss.item(), 100 * correct / total, 100*correct_p/val_patient_num]+confusion_matrix.reshape(1,-1)[0].tolist())\n",
        "        \n",
        "        # if val_loss<best_val, update best_model\n",
        "        if running_loss/total<best_val:\n",
        "            best_model=copy.deepcopy(model).state_dict()\n",
        "            best_val=running_loss/total\n",
        "            \n",
        "        # if val acc>best_val_acc, update best_model_acc\n",
        "        if correct/total>best_val_acc:\n",
        "            best_model_acc=copy.deepcopy(model).state_dict()\n",
        "            best_val_acc=correct/total\n",
        "        \n",
        "    # Best_model testing\n",
        "    confusion_matrix = torch.zeros(2,2)\n",
        "    correct,total,correct_p = 0,0,0\n",
        "    running_loss = 0.0\n",
        "    model.load_state_dict(best_model)\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels, _, ratios = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            ratios = ratios.float()\n",
        "            ratios = ratios.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.sum()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            correct_p += ((predicted == labels).float()*ratios).sum().item() \n",
        "            for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "    print('Accuracy of the network on the test images for the best model: %d %%' % (100 * correct / total))\n",
        "    print(correct_p/test_patient_num)\n",
        "    test_loss.append([running_loss.item(),100 * correct / total,100*correct_p/test_patient_num]+\n",
        "                        confusion_matrix.reshape(1,-1)[0].tolist())\n",
        "    print('Finished Training')\n",
        "    # Save outputs\n",
        "    if not experiment_name:\n",
        "        execution_time = datetime.now().strftime('%d-%m-%Y-%H-%M')\n",
        "        experiment_name = execution_time\n",
        "    experiment_folder = os.path.join(save_dir,experiment_name,'fold'+str(test_foldid))\n",
        "    if not os.path.exists(experiment_folder): os.makedirs(experiment_folder)\n",
        "    torch.save(model.state_dict(), os.path.join(experiment_folder,'model.pt'))\n",
        "    torch.save(best_model, os.path.join(experiment_folder,\"best_val__model.pt\"))\n",
        "    torch.save(best_model_acc, os.path.join(experiment_folder,\"best_val_acc_model.pt\"))\n",
        "    pd.DataFrame(train_loss).to_csv(os.path.join(experiment_folder,'train.csv'),index=False)\n",
        "    pd.DataFrame(val_loss).to_csv(os.path.join(experiment_folder,'val.csv'),index=False)\n",
        "    pd.DataFrame(test_loss).to_csv(os.path.join(experiment_folder,'test.csv'),index=False)\n",
        "    \n",
        "    \"\"\"go\"\"\"\n",
        "    print('Acompanhamento do dataset!!')\n",
        "    # head -> https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html\n",
        "    print(poly_df.head(25))\n",
        "    print('\\n\\n\\n\\depois de algumas linhas:')\n",
        "    print(poly_df.loc[poly_df.image_name == poly_df.image_name[3], \"image_path\"])\n",
        "\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "1B1MCbrm6agx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Start"
      ],
      "metadata": {
        "id": "LZ37p3vh8TPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read JSON files"
      ],
      "metadata": {
        "id": "aPXthXcL_56s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jsonfiles = get_json_files()\n",
        "print(f'Patients Num -> {len(jsonfiles)}')\n",
        "data = get_data(jsonfiles)\n",
        "print(f'Imagens Num -> {len(data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd9eSj0-oH5D",
        "outputId": "038455f4-4eb4-4aef-b9d6-1d4a94f09a60"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patients Num -> 225\n",
            "Numero de imagens -> \n",
            "Colseup: 732\n",
            "All: 1231\n",
            "Panoramicas: 499\n",
            "Imagens Num -> 1205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWj0KH_rvJ1l",
        "outputId": "5640f894-67ef-445e-e801-370e69914518"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['006CJ', 'Patient-006CJ', 'Image-006CJ1-trunk-panoramic', 'https://patientimage.blob.core.windows.net/processedimages/Patient-006CJ/Image-006CJ1-trunk-panoramic.jpg', '/content/drive/MyDrive/Colab Notebooks/dataset/skinreader/BDHansen/closeup/Image-006CJ1-trunk-panoramic.jpg', 'leprosy', 'all', 'panoramic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data, columns=['StudyID', 'patient_id', 'image_name', 'filename', 'image_path', 'patient_leprosy', 'local_dist', 'type'])\n",
        "df.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lmd9htjhuNQr",
        "outputId": "1955e8d3-14d4-42ee-96d5-218edb112e0f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     StudyID     patient_id                     image_name  \\\n",
              "1200   228FM  Patient-228FM  Image-228FM5-llimbs-panoramic   \n",
              "1201   225MA  Patient-225MA     Image-225MA2-trunk-closeup   \n",
              "1202   225MA  Patient-225MA  Image-225MA1-llimbs-panoramic   \n",
              "1203   225MA  Patient-225MA    Image-225MA1-llimbs-closeup   \n",
              "1204   225MA  Patient-225MA   Image-225MA2-trunk-panoramic   \n",
              "\n",
              "                                               filename  \\\n",
              "1200  https://patientimage.blob.core.windows.net/pro...   \n",
              "1201  https://patientimage.blob.core.windows.net/pro...   \n",
              "1202  https://patientimage.blob.core.windows.net/pro...   \n",
              "1203  https://patientimage.blob.core.windows.net/pro...   \n",
              "1204  https://patientimage.blob.core.windows.net/pro...   \n",
              "\n",
              "                                             image_path   patient_leprosy  \\\n",
              "1200  /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
              "1201  /content/drive/MyDrive/Colab Notebooks/dataset...  other_dermotosis   \n",
              "1202  /content/drive/MyDrive/Colab Notebooks/dataset...  other_dermotosis   \n",
              "1203  /content/drive/MyDrive/Colab Notebooks/dataset...  other_dermotosis   \n",
              "1204  /content/drive/MyDrive/Colab Notebooks/dataset...  other_dermotosis   \n",
              "\n",
              "     local_dist       type  \n",
              "1200        all  panoramic  \n",
              "1201        all    closeup  \n",
              "1202        all  panoramic  \n",
              "1203        all    closeup  \n",
              "1204        all  panoramic  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-deab7c40-e4bc-400f-943b-8dad0ce83dfc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>StudyID</th>\n",
              "      <th>patient_id</th>\n",
              "      <th>image_name</th>\n",
              "      <th>filename</th>\n",
              "      <th>image_path</th>\n",
              "      <th>patient_leprosy</th>\n",
              "      <th>local_dist</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1200</th>\n",
              "      <td>228FM</td>\n",
              "      <td>Patient-228FM</td>\n",
              "      <td>Image-228FM5-llimbs-panoramic</td>\n",
              "      <td>https://patientimage.blob.core.windows.net/pro...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>leprosy</td>\n",
              "      <td>all</td>\n",
              "      <td>panoramic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1201</th>\n",
              "      <td>225MA</td>\n",
              "      <td>Patient-225MA</td>\n",
              "      <td>Image-225MA2-trunk-closeup</td>\n",
              "      <td>https://patientimage.blob.core.windows.net/pro...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>other_dermotosis</td>\n",
              "      <td>all</td>\n",
              "      <td>closeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1202</th>\n",
              "      <td>225MA</td>\n",
              "      <td>Patient-225MA</td>\n",
              "      <td>Image-225MA1-llimbs-panoramic</td>\n",
              "      <td>https://patientimage.blob.core.windows.net/pro...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>other_dermotosis</td>\n",
              "      <td>all</td>\n",
              "      <td>panoramic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1203</th>\n",
              "      <td>225MA</td>\n",
              "      <td>Patient-225MA</td>\n",
              "      <td>Image-225MA1-llimbs-closeup</td>\n",
              "      <td>https://patientimage.blob.core.windows.net/pro...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>other_dermotosis</td>\n",
              "      <td>all</td>\n",
              "      <td>closeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1204</th>\n",
              "      <td>225MA</td>\n",
              "      <td>Patient-225MA</td>\n",
              "      <td>Image-225MA2-trunk-panoramic</td>\n",
              "      <td>https://patientimage.blob.core.windows.net/pro...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>other_dermotosis</td>\n",
              "      <td>all</td>\n",
              "      <td>panoramic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-deab7c40-e4bc-400f-943b-8dad0ce83dfc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-deab7c40-e4bc-400f-943b-8dad0ce83dfc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-deab7c40-e4bc-400f-943b-8dad0ce83dfc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BAEC-By0vqOU",
        "outputId": "2ed6a163-0bbc-4803-a3fb-f79439951165"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  StudyID     patient_id                     image_name  \\\n",
              "0   006CJ  Patient-006CJ   Image-006CJ1-trunk-panoramic   \n",
              "1   006CJ  Patient-006CJ        Image-006CJ1-trunk-edge   \n",
              "2   006CJ  Patient-006CJ     Image-006CJ1-trunk-closeup   \n",
              "3   002IR  Patient-002IR    Image-002IR2-ulimbs-closeup   \n",
              "4   002IR  Patient-002IR  Image-002IR2-ulimbs-panoramic   \n",
              "\n",
              "                                            filename  \\\n",
              "0  https://patientimage.blob.core.windows.net/pro...   \n",
              "1  https://patientimage.blob.core.windows.net/pro...   \n",
              "2  https://patientimage.blob.core.windows.net/pro...   \n",
              "3  https://patientimage.blob.core.windows.net/pro...   \n",
              "4  https://patientimage.blob.core.windows.net/pro...   \n",
              "\n",
              "                                          image_path patient_leprosy  \\\n",
              "0  /content/drive/MyDrive/Colab Notebooks/dataset...         leprosy   \n",
              "1  /content/drive/MyDrive/Colab Notebooks/dataset...         leprosy   \n",
              "2  /content/drive/MyDrive/Colab Notebooks/dataset...         leprosy   \n",
              "3  /content/drive/MyDrive/Colab Notebooks/dataset...         leprosy   \n",
              "4  /content/drive/MyDrive/Colab Notebooks/dataset...         leprosy   \n",
              "\n",
              "  local_dist       type  \n",
              "0        all  panoramic  \n",
              "1        all       edge  \n",
              "2        all    closeup  \n",
              "3        all    closeup  \n",
              "4        all  panoramic  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aef72f59-46ea-47bb-b9ec-a87116496fb8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>StudyID</th>\n",
              "      <th>patient_id</th>\n",
              "      <th>image_name</th>\n",
              "      <th>filename</th>\n",
              "      <th>image_path</th>\n",
              "      <th>patient_leprosy</th>\n",
              "      <th>local_dist</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>006CJ</td>\n",
              "      <td>Patient-006CJ</td>\n",
              "      <td>Image-006CJ1-trunk-panoramic</td>\n",
              "      <td>https://patientimage.blob.core.windows.net/pro...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>leprosy</td>\n",
              "      <td>all</td>\n",
              "      <td>panoramic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>006CJ</td>\n",
              "      <td>Patient-006CJ</td>\n",
              "      <td>Image-006CJ1-trunk-edge</td>\n",
              "      <td>https://patientimage.blob.core.windows.net/pro...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>leprosy</td>\n",
              "      <td>all</td>\n",
              "      <td>edge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>006CJ</td>\n",
              "      <td>Patient-006CJ</td>\n",
              "      <td>Image-006CJ1-trunk-closeup</td>\n",
              "      <td>https://patientimage.blob.core.windows.net/pro...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>leprosy</td>\n",
              "      <td>all</td>\n",
              "      <td>closeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>002IR</td>\n",
              "      <td>Patient-002IR</td>\n",
              "      <td>Image-002IR2-ulimbs-closeup</td>\n",
              "      <td>https://patientimage.blob.core.windows.net/pro...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>leprosy</td>\n",
              "      <td>all</td>\n",
              "      <td>closeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>002IR</td>\n",
              "      <td>Patient-002IR</td>\n",
              "      <td>Image-002IR2-ulimbs-panoramic</td>\n",
              "      <td>https://patientimage.blob.core.windows.net/pro...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>leprosy</td>\n",
              "      <td>all</td>\n",
              "      <td>panoramic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aef72f59-46ea-47bb-b9ec-a87116496fb8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aef72f59-46ea-47bb-b9ec-a87116496fb8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aef72f59-46ea-47bb-b9ec-a87116496fb8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training - Data Augmentation\n",
        "\n",
        "\n",
        "*   Barbiere\n",
        "*   All augmentation types\n",
        "\n"
      ],
      "metadata": {
        "id": "8nCzprIT-2yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformations = {\n",
        "    \"type\": \"r_claro\", # \"r_barbiere\", \"p_barbiere\", \"r_claro\", \"p_claro\"\n",
        "    \"rotations\": True,\n",
        "    \"translation\": True,\n",
        "    \"shear\": True,\n",
        "    \"flip\": True,\n",
        "    \"zoom\": True,\n",
        "    \"jitter\": True\n",
        "}"
      ],
      "metadata": {
        "id": "YktKS16Z2Aph"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_model(\n",
        "    save_dir='/content/drive/MyDrive/Colab Notebooks/skinreadermodels/barbieri2022-info', # models and result tables will saved in 'save_dir/experiment_name'. 'save_dir' must exists!\n",
        "    dir_name='/content/drive/MyDrive/Colab Notebooks/dataset/skinreader/BDHansen', # all images are in dir_name/dataset_name\n",
        "    \n",
        "    poly_df = df,\n",
        "    id=0, # cuda id for parallel experiments\n",
        "    experiment_name = None, # if None, experiment_name will be the date when exp is finished\n",
        "    lr=.003, # learning rate\n",
        "    wd=0, # weight decay\n",
        "    batch_size=16,\n",
        "    tuning='tune_all', # default is to train the whole neural net. Otherwise train the last layer only. \n",
        "    class_weight=\"no\", # not apply class_weight to the loss. Otherwise do apply. \n",
        "    sample_weight=\"no\",# not apply sample_weight to the loss. Otherwise do apply. \n",
        "    num_epoch=10, # num of epochs\n",
        "    model_name='resnet50',# the other option 'inceptionv4'\n",
        "    no_augmentation=False, # if TRUE -> no augmentation at all\n",
        "    transformations=transformations,\n",
        "    change_aspect_ratio=None, # whether to change aspect ratio\n",
        "    scale=.6, # scaling factor for data augmentation\n",
        "    \n",
        "    dataset_name='all_closeup_panoramic', # all images are in dir_name/dataset_name\n",
        "    type_spec=None,# If None, use all three types of images. If ='closeup'/'panoramic'/'edge', only use the specific type of images for model training/validation/testing.\n",
        "    optim_type=\"SGD\", # otherwise use \"Adam\" as the optimizer\n",
        "    num_fold=6, # Split all patients into 6 folds. The last folds will be used to validate Model 3 later. The first 5 folds will be used for 5-fold cross-validation. \n",
        "    test_foldid=0 # index of the validation fold. 0-4 only.\n",
        ")"
      ],
      "metadata": {
        "id": "eQm94GdQ8STk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fc778460cc6b4a62bea97546f16da54a",
            "a911261923394d3fbb7e5c97231786c7",
            "659e9daed75e41739ec4a1295275dec1",
            "37e7ac0a08454ea1abc62367209437cd",
            "b7fbb39229d4462a80de5cb35ef27341",
            "dfa0eb9fb4d04aec9278768d1b6e4cdd",
            "de2df4d18b614ea6962ec61b0b5c7408",
            "4dd434caaccb4f2c903cbbee7460c39e",
            "b7c0b05a2b334c9282b60c10bb82e4e8",
            "173df3e895dd4e7ab54c9c353b84e721",
            "93806b766fb04ebb916cc131a98ca8e3"
          ]
        },
        "outputId": "82760df7-20f5-4ed9-d326-c3187a5b8835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc778460cc6b4a62bea97546f16da54a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type model: <class 'torchvision.models.resnet.ResNet'>, \n",
            "model.input_size[1/: 224\n",
            "\n",
            "\n",
            "[1] loss: 436.460\n",
            "Accuracy of the network on the training images: 47 %\n",
            "0.4453604854991503\n",
            "Accuracy of the network on the val images: 50 %\n",
            "0.44725528959598804\n",
            "Accuracy of the network on the test images: 50 %\n",
            "0.4117646988700418\n",
            "[2] loss: 357.001\n",
            "Accuracy of the network on the training images: 60 %\n",
            "0.618398113785503\n",
            "Accuracy of the network on the val images: 47 %\n",
            "0.5224852859973907\n",
            "Accuracy of the network on the test images: 57 %\n",
            "0.6571895515217501\n",
            "[3] loss: 351.652\n",
            "Accuracy of the network on the training images: 62 %\n",
            "0.592290399787582\n",
            "Accuracy of the network on the val images: 48 %\n",
            "0.45439739028612774\n",
            "Accuracy of the network on the test images: 47 %\n",
            "0.43251634608296785\n",
            "[4] loss: 316.573\n",
            "Accuracy of the network on the training images: 72 %\n",
            "0.6961731649238929\n",
            "Accuracy of the network on the val images: 46 %\n",
            "0.4814803754496906\n",
            "Accuracy of the network on the test images: 57 %\n",
            "0.5996965590645286\n",
            "[5] loss: 294.771\n",
            "Accuracy of the network on the training images: 73 %\n",
            "0.7112417137511423\n",
            "Accuracy of the network on the val images: 45 %\n",
            "0.4735257360670302\n",
            "Accuracy of the network on the test images: 62 %\n",
            "0.639122328337501\n",
            "[6] loss: 305.203\n",
            "Accuracy of the network on the training images: 71 %\n",
            "0.65162507089499\n",
            "Accuracy of the network on the val images: 52 %\n",
            "0.5371987927291129\n",
            "Accuracy of the network on the test images: 56 %\n",
            "0.5278945004238802\n",
            "[7] loss: 399.591\n",
            "Accuracy of the network on the training images: 60 %\n",
            "0.5552905591848855\n",
            "Accuracy of the network on the val images: 55 %\n",
            "0.5009317617449496\n",
            "Accuracy of the network on the test images: 50 %\n",
            "0.44607843370998607\n",
            "[8] loss: 326.428\n",
            "Accuracy of the network on the training images: 71 %\n",
            "0.7009969811851733\n",
            "Accuracy of the network on the val images: 47 %\n",
            "0.5061494008534484\n",
            "Accuracy of the network on the test images: 63 %\n",
            "0.6909430623054504\n",
            "[9] loss: 239.016\n",
            "Accuracy of the network on the training images: 81 %\n",
            "0.7842041777673169\n",
            "Accuracy of the network on the val images: 47 %\n",
            "0.5032945767872863\n",
            "Accuracy of the network on the test images: 56 %\n",
            "0.5391223237794989\n",
            "[10] loss: 305.677\n",
            "Accuracy of the network on the training images: 72 %\n",
            "0.7132745658125833\n",
            "Accuracy of the network on the val images: 47 %\n",
            "0.5141565574126111\n",
            "Accuracy of the network on the test images: 56 %\n",
            "0.6380485629334169\n",
            "Finished Training\n",
            "Acompanhamento do dataset!!\n",
            "   StudyID     patient_id                     image_name  \\\n",
            "0    006CJ  Patient-006CJ   Image-006CJ1-trunk-panoramic   \n",
            "1    006CJ  Patient-006CJ        Image-006CJ1-trunk-edge   \n",
            "2    006CJ  Patient-006CJ     Image-006CJ1-trunk-closeup   \n",
            "3    002IR  Patient-002IR    Image-002IR2-ulimbs-closeup   \n",
            "4    002IR  Patient-002IR  Image-002IR2-ulimbs-panoramic   \n",
            "5    004MF  Patient-004MF   Image-004MF-llimbs-panoramic   \n",
            "6    004MF  Patient-004MF     Image-004MF-llimbs-closeup   \n",
            "7    008JS  Patient-008JS   Image-008JS1-trunk-panoramic   \n",
            "8    008JS  Patient-008JS    Image-008JS2-ulimbs-closeup   \n",
            "9    008JS  Patient-008JS    Image-008JS3-ulimbs-closeup   \n",
            "10   008JS  Patient-008JS  Image-008JS2-ulimbs-panoramic   \n",
            "11   008JS  Patient-008JS     Image-008JS1-trunk-closeup   \n",
            "12   008JS  Patient-008JS  Image-008JS3-ulimbs-panoramic   \n",
            "13   001WN  Patient-001WN    Image-001WN3-ulimbs-closeup   \n",
            "14   001WN  Patient-001WN   Image-001WN2-trunk-panoramic   \n",
            "15   001WN  Patient-001WN     Image-001WN2-trunk-closeup   \n",
            "16   005MO  Patient-005MO      Image-005MO-trunk-closeup   \n",
            "18   007GB  Patient-007GB    Image-007GB1-ulimbs-closeup   \n",
            "19   007GB  Patient-007GB  Image-007GB1-ulimbs-panoramic   \n",
            "20   003AL  Patient-003AL    Image-003AL1-ulimbs-closeup   \n",
            "21   003AL  Patient-003AL  Image-003AL1-ulimbs-panoramic   \n",
            "22   014IS  Patient-014IS  Image-014IS1-ulimbs-panoramic   \n",
            "23   014IS  Patient-014IS    Image-014IS3-ulimbs-closeup   \n",
            "24   014IS  Patient-014IS  Image-014IS3-ulimbs-panoramic   \n",
            "25   014IS  Patient-014IS      Image-014IS4-face-closeup   \n",
            "\n",
            "                                             filename  \\\n",
            "0   https://patientimage.blob.core.windows.net/pro...   \n",
            "1   https://patientimage.blob.core.windows.net/pro...   \n",
            "2   https://patientimage.blob.core.windows.net/pro...   \n",
            "3   https://patientimage.blob.core.windows.net/pro...   \n",
            "4   https://patientimage.blob.core.windows.net/pro...   \n",
            "5   https://patientimage.blob.core.windows.net/pro...   \n",
            "6   https://patientimage.blob.core.windows.net/pro...   \n",
            "7   https://patientimage.blob.core.windows.net/pro...   \n",
            "8   https://patientimage.blob.core.windows.net/pro...   \n",
            "9   https://patientimage.blob.core.windows.net/pro...   \n",
            "10  https://patientimage.blob.core.windows.net/pro...   \n",
            "11  https://patientimage.blob.core.windows.net/pro...   \n",
            "12  https://patientimage.blob.core.windows.net/pro...   \n",
            "13  https://patientimage.blob.core.windows.net/pro...   \n",
            "14  https://patientimage.blob.core.windows.net/pro...   \n",
            "15  https://patientimage.blob.core.windows.net/pro...   \n",
            "16  https://patientimage.blob.core.windows.net/pro...   \n",
            "18  https://patientimage.blob.core.windows.net/pro...   \n",
            "19  https://patientimage.blob.core.windows.net/pro...   \n",
            "20  https://patientimage.blob.core.windows.net/pro...   \n",
            "21  https://patientimage.blob.core.windows.net/pro...   \n",
            "22  https://patientimage.blob.core.windows.net/pro...   \n",
            "23  https://patientimage.blob.core.windows.net/pro...   \n",
            "24  https://patientimage.blob.core.windows.net/pro...   \n",
            "25  https://patientimage.blob.core.windows.net/pro...   \n",
            "\n",
            "                                           image_path   patient_leprosy  \\\n",
            "0   /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "1   /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "2   /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "3   /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "4   /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "5   /content/drive/MyDrive/Colab Notebooks/dataset...  other_dermotosis   \n",
            "6   /content/drive/MyDrive/Colab Notebooks/dataset...  other_dermotosis   \n",
            "7   /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "8   /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "9   /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "10  /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "11  /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "12  /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "13  /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "14  /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "15  /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "16  /content/drive/MyDrive/Colab Notebooks/dataset...  other_dermotosis   \n",
            "18  /content/drive/MyDrive/Colab Notebooks/dataset...  other_dermotosis   \n",
            "19  /content/drive/MyDrive/Colab Notebooks/dataset...  other_dermotosis   \n",
            "20  /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "21  /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "22  /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "23  /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "24  /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "25  /content/drive/MyDrive/Colab Notebooks/dataset...           leprosy   \n",
            "\n",
            "   local_dist       type  sample_weight  sample_freq_ratio  \n",
            "0         all  panoramic       5.666667           0.333333  \n",
            "1         all       edge       5.666667           0.333333  \n",
            "2         all    closeup       5.666667           0.333333  \n",
            "3         all    closeup       8.500000           0.500000  \n",
            "4         all  panoramic       8.500000           0.500000  \n",
            "5         all  panoramic       8.500000           0.500000  \n",
            "6         all    closeup       8.500000           0.500000  \n",
            "7         all  panoramic       2.833333           0.166667  \n",
            "8         all    closeup       2.833333           0.166667  \n",
            "9         all    closeup       2.833333           0.166667  \n",
            "10        all  panoramic       2.833333           0.166667  \n",
            "11        all    closeup       2.833333           0.166667  \n",
            "12        all  panoramic       2.833333           0.166667  \n",
            "13        all    closeup       5.666667           0.333333  \n",
            "14        all  panoramic       5.666667           0.333333  \n",
            "15        all    closeup       5.666667           0.333333  \n",
            "16        all    closeup      17.000000           1.000000  \n",
            "18        all    closeup       8.500000           0.500000  \n",
            "19        all  panoramic       8.500000           0.500000  \n",
            "20        all    closeup       8.500000           0.500000  \n",
            "21        all  panoramic       8.500000           0.500000  \n",
            "22        all  panoramic       2.833333           0.166667  \n",
            "23        all    closeup       2.833333           0.166667  \n",
            "24        all  panoramic       2.833333           0.166667  \n",
            "25        all    closeup       2.833333           0.166667  \n",
            "\n",
            "\n",
            "\n",
            "\\depois de algumas linhas:\n",
            "3    /content/drive/MyDrive/Colab Notebooks/dataset...\n",
            "Name: image_path, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training - Data Augmentation\n",
        "\n",
        "\n",
        "*   Claro\n",
        "*   All augmentation types\n",
        "*   Random\n"
      ],
      "metadata": {
        "id": "KPVuNzix_Iy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformations = {\n",
        "    \"type\": \"r_claro\", # \"r_barbiere\", \"p_barbiere\", \"r_claro\", \"p_claro\"\n",
        "    \"rotation\": True,\n",
        "    \"translation\": True,\n",
        "    \"shear\": True,\n",
        "    \"flip\": True,\n",
        "    \"zoom\": True,\n",
        "    \"jitter\": True\n",
        "}"
      ],
      "metadata": {
        "id": "aYxeXA_g-zin"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_model(\n",
        "    save_dir='/content/drive/MyDrive/Colab Notebooks/skinreadermodels/barbieri2022-info', # models and result tables will saved in 'save_dir/experiment_name'. 'save_dir' must exists!\n",
        "    dir_name='/content/drive/MyDrive/Colab Notebooks/dataset/skinreader/BDHansen', # all images are in dir_name/dataset_name\n",
        "    \n",
        "    poly_df = df,\n",
        "    id=0, # cuda id for parallel experiments\n",
        "    experiment_name = None, # if None, experiment_name will be the date when exp is finished\n",
        "    lr=.003, # learning rate\n",
        "    wd=0, # weight decay\n",
        "    batch_size=16,\n",
        "    tuning='tune_all', # default is to train the whole neural net. Otherwise train the last layer only. \n",
        "    class_weight=\"no\", # not apply class_weight to the loss. Otherwise do apply. \n",
        "    sample_weight=\"no\",# not apply sample_weight to the loss. Otherwise do apply. \n",
        "    num_epoch=10, # num of epochs\n",
        "    model_name='resnet50',# the other option 'inceptionv4'\n",
        "    no_augmentation=False, # if TRUE -> no augmentation at all\n",
        "    transformations=transformations,\n",
        "    change_aspect_ratio=None, # whether to change aspect ratio\n",
        "    scale=.6, # scaling factor for data augmentation\n",
        "    \n",
        "    dataset_name='all_closeup_panoramic', # all images are in dir_name/dataset_name\n",
        "    type_spec=None,# If None, use all three types of images. If ='closeup'/'panoramic'/'edge', only use the specific type of images for model training/validation/testing.\n",
        "    optim_type=\"SGD\", # otherwise use \"Adam\" as the optimizer\n",
        "    num_fold=6, # Split all patients into 6 folds. The last folds will be used to validate Model 3 later. The first 5 folds will be used for 5-fold cross-validation. \n",
        "    test_foldid=0 # index of the validation fold. 0-4 only.\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "de46d819bcee43fd9be041ce44e021a2",
            "7697eddb7e4c478fbb5f1c9499a1c8bb",
            "918d64379b65418c92ff925e88c3a59f",
            "cf33f188a85743ca82f017c36d963425",
            "31c7eee3e8ea412691ff496528bd3a89",
            "23844d331cd347a386c974565417bfa2",
            "ae98c2557e2a4a13bc0db0ec0984b704",
            "68d788d1f43f471ba91df8b665f7cc5c",
            "dc496956bff84473b95b3800e4baae06",
            "ae6770b09b494bc2a83e82d9a4f47a3e",
            "42e6611f29b44734ab34def1c560a1d1"
          ]
        },
        "id": "ioL8f9Ue9c1T",
        "outputId": "1d11eca2-1405-41f5-f9ac-cbc978bf87f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de46d819bcee43fd9be041ce44e021a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type model: <class 'torchvision.models.resnet.ResNet'>, \n",
            "model.input_size[1/: 224\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}